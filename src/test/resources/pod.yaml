apiVersion: v1
kind: Pod
metadata:
  annotations:
    cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
    k8s.v1.cni.cncf.io/network-status: |-
      [{
          "name": "openshift-sdn",
          "interface": "eth0",
          "ips": [
              "10.131.0.169"
          ],
          "default": true,
          "dns": {}
      }]
    openshift.io/scc: restricted-v2
    seccomp.security.alpha.kubernetes.io/pod: runtime/default
  generateName: ds-pipeline-persistenceagent-sample-cbd7d67f8-
  labels:
    app: ds-pipeline-persistenceagent-sample
    component: data-science-pipelines
    dspa: sample
    pod-template-hash: cbd7d67f8
  name: ds-pipeline-persistenceagent-sample-cbd7d67f8-tmqw7
  namespace: kubeflow
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: ds-pipeline-persistenceagent-sample-cbd7d67f8
    uid: 0a75169f-8988-402c-a919-db62568de2d6
  resourceVersion: "43007563"
  uid: ab1dc36d-c441-49ce-96c8-9f85b2e83426
spec:
  containers:
  - command:
    - persistence_agent
    - --logtostderr=true
    - --ttlSecondsAfterWorkflowFinish=86400
    - --numWorker=2
    - --mlPipelineAPIServerName=ds-pipeline-sample
    - --namespace=kubeflow
    - --mlPipelineServiceHttpPort=8888
    - --mlPipelineServiceGRPCPort=8887
    env:
    - name: NAMESPACE
      value: kubeflow
    - name: TTL_SECONDS_AFTER_WORKFLOW_FINISH
      value: "86400"
    - name: NUM_WORKERS
      value: "2"
    - name: KUBEFLOW_USERID_HEADER
      value: kubeflow-userid
    - name: KUBEFLOW_USERID_PREFIX
    - name: EXECUTIONTYPE
      value: PipelineRun
    image: quay.io/modh/odh-ml-pipelines-persistenceagent-container:v1.18.0-8
    imagePullPolicy: IfNotPresent
    livenessProbe:
      exec:
        command:
        - test
        - -x
        - persistence_agent
      failureThreshold: 3
      initialDelaySeconds: 30
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 2
    name: ds-pipeline-persistenceagent
    readinessProbe:
      exec:
        command:
        - test
        - -x
        - persistence_agent
      failureThreshold: 3
      initialDelaySeconds: 3
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 2
    resources:
      limits:
        cpu: 250m
        memory: 1Gi
      requests:
        cpu: 120m
        memory: 500Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      runAsNonRoot: true
      runAsUser: 1000710000
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-btmf9
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: hbelmiro-5-d95dd-worker-0-wccsf
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 1000710000
    seLinuxOptions:
      level: s0:c27,c4
    seccompProfile:
      type: RuntimeDefault
  serviceAccount: ds-pipeline-persistenceagent-sample
  serviceAccountName: ds-pipeline-persistenceagent-sample
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
  volumes:
  - name: kube-api-access-btmf9
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
      - configMap:
          items:
          - key: service-ca.crt
            path: service-ca.crt
          name: openshift-service-ca.crt
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-02-26T19:48:09Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-02-26T19:48:15Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-02-26T19:48:15Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-02-26T19:48:09Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://d000b00bfc51ad9861f8ac8407a4c850077d5848512675d0b4509069b6c2cad2
    image: quay.io/modh/odh-ml-pipelines-persistenceagent-container:v1.18.0-8
    imageID: quay.io/modh/odh-ml-pipelines-persistenceagent-container@sha256:9f9a2c9a90681be072320b370d0ca419c38e3309c621754bc033ac1e0e3d9634
    lastState: {}
    name: ds-pipeline-persistenceagent
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-02-26T19:48:11Z"
  hostIP: 192.168.1.120
  phase: Running
  podIP: 10.131.0.169
  podIPs:
  - ip: 10.131.0.169
  qosClass: Burstable
  startTime: "2024-02-26T19:48:09Z"